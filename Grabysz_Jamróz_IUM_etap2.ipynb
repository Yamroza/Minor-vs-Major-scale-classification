{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineering\n",
    "# Project - stage II\n",
    "**Authors:**\n",
    "- Marcin Grabysz\n",
    "- Aleksandra Jamr√≥z\n",
    "\n",
    "**Task**\n",
    "\n",
    "Our song database is quite rich - they are described by many interesting parameters. Why nobody tagged if they are in major or minor scale so far? We have to change it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic model\n",
    "\n",
    "In our previous analysis we tried simple XGBoost approach. It will serve as our basic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 25688 entries, 0 to 25928\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   danceability      25688 non-null  float64\n",
      " 1   energy            25688 non-null  float64\n",
      " 2   key               25688 non-null  int64  \n",
      " 3   mode              25688 non-null  float64\n",
      " 4   loudness          25688 non-null  float64\n",
      " 5   speechiness       25688 non-null  float64\n",
      " 6   acousticness      25688 non-null  float64\n",
      " 7   instrumentalness  25688 non-null  float64\n",
      " 8   liveness          25688 non-null  float64\n",
      " 9   valence           25688 non-null  float64\n",
      " 10  tempo             25688 non-null  float64\n",
      " 11  time_signature    25688 non-null  int64  \n",
      "dtypes: float64(10), int64(2)\n",
      "memory usage: 2.5 MB\n"
     ]
    }
   ],
   "source": [
    "# Data loading\n",
    "# exactly the same as in stage I\n",
    "\n",
    "df = pd.read_json(\"IUM23L_Zad_08_03_v2/tracks.jsonl\", lines=True)\n",
    "df.drop_duplicates(subset=(\"name\", \"id_artist\"), inplace=True)\n",
    "df = df.loc[df['mode'].notnull()]\n",
    "df = df.drop(columns=[\"id\", \"name\", \"popularity\", \"explicit\", \"duration_ms\", \"id_artist\", \"release_date\"])\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import sys\n",
    "\n",
    "sys.modules['sklearn.externals.joblib'] = joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.externals.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexternals\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mjoblib\u001b[39;00m \u001b[39mimport\u001b[39;00m dump, load\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplit\u001b[39m(df, file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m      4\u001b[0m     \u001b[39m# dividing into X and y \u001b[39;00m\n\u001b[1;32m      6\u001b[0m     X_final \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mmode\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.externals.joblib'"
     ]
    }
   ],
   "source": [
    "from sklearn.externals.joblib import dump, load\n",
    "\n",
    "def split(df, file=None):\n",
    "    # dividing into X and y \n",
    "\n",
    "    X_final = df.drop(columns=[\"mode\"])\n",
    "    y_final = df[\"mode\"]\n",
    "\n",
    "    # scaling the data and train test split\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_final)\n",
    "    if file:\n",
    "        dump(scaler, file, compress=True)\n",
    "\n",
    "    X_scaled = scaler.transform(X_final)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_final, test_size=0.2, random_state=32)\n",
    "    return X_train, X_test, y_train, y_test, X_scaled, y_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "    print(\"Accuracy: \", round(acc*100, 4), \"%\")\n",
    "    print(\"Recall: \", round(recall*100, 4), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_val_scores(model, X, y, folds=3):\n",
    "    print(\"Cross validation scores: \", cross_val_score(model, X, y, cv=folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, X_final, y_final = split(df, 'basic.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tree = xgb.XGBClassifier()\n",
    "# saving basic model:\n",
    "# xgb_tree.fit(X_train, y_train)\n",
    "# xgb_tree.save_model('basic_model.json')\n",
    "get_scores(xgb_tree, X_train, y_train, X_test, y_test)\n",
    "get_cross_val_scores(xgb.XGBClassifier(), X_final, y_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key', 0.28115368),\n",
       " ('speechiness', 0.08026361),\n",
       " ('acousticness', 0.07867877),\n",
       " ('loudness', 0.07853775),\n",
       " ('danceability', 0.07111322),\n",
       " ('liveness', 0.07092597),\n",
       " ('mode', 0.07055699),\n",
       " ('energy', 0.07052805),\n",
       " ('valence', 0.07020085),\n",
       " ('instrumentalness', 0.06541826),\n",
       " ('tempo', 0.06262291)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature importances:\n",
    "\n",
    "sorted(\n",
    "    list(zip(df.columns, xgb_tree.feature_importances_)),\n",
    "    key=lambda t: t[1],\n",
    "    reverse=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main column having impact on our prediction is key. It is not a surprise for us, as we conducted research in the first stage of the project and expected exactly this. Rest of the columns have similar impact on the model, starting with speechiness and ending with tempo. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic model - results\n",
    "\n",
    "Data preprocession for basic model was not advanced. Steps which were taken:\n",
    "- deletion of columns which we think are useless\n",
    "- data scaling.\n",
    "\n",
    "Nothing more was done. More steps will be introduced in further modelling.\n",
    "\n",
    "Highest score so far is **69,33% accuracy**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced model\n",
    "\n",
    "We will try more developed approaches.\n",
    "Plans:\n",
    "- using logistic regressor instead of xgboost\n",
    "- changing class balance (making in more 50/50 between categories)\n",
    "- choosing appropriate columns\n",
    "- modyfing column structure if needed\n",
    "- hyperparameter tuning.\n",
    "\n",
    "Following cells are just some of our approaches. Including all of the attempts would take too much space and affect the clarity of the report.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  68.0226 %\n",
      "Recall:  99.4531 %\n"
     ]
    }
   ],
   "source": [
    "l_regresson = LogisticRegression()\n",
    "get_scores(l_regresson, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  68.042 %\n",
      "Recall:  99.4819 %\n"
     ]
    }
   ],
   "source": [
    "l_regresson = LogisticRegression(C = 0.5)\n",
    "get_scores(l_regresson, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  67.6139 %\n",
      "Recall:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "weights = {\n",
    "    0: 0,\n",
    "    1: 1.3\n",
    "}\n",
    "\n",
    "l_regresson = LogisticRegression(C = 0.5, class_weight=weights)\n",
    "get_scores(l_regresson, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After several experiments with following parameters:\n",
    "- penalty\n",
    "- tol\n",
    "- C\n",
    "- solver\n",
    "- warm_start\n",
    "- class_weights\n",
    "\n",
    "only changing C value had positive impact of the score. Even though, the accuracy was lower than xgboost, so we resigned from further develpoment of logistic regressor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing classes\n",
    "\n",
    "66% of the dataset are major tracks, while the rest (34%) are minor tracks. Those classes are unbalanced and it shows in basic approach. As we checked in previous stage (copied from the analyses):\n",
    "\n",
    "*Test set*\n",
    "\n",
    "*Major tracks in test set:  3396.0 -> Part of dataset:  66.1 %*\n",
    "\n",
    "*Minor tracks in test set:  1742.0 -> Part of dataset:  33.9 %*\n",
    "\n",
    "*Prediction*\n",
    "\n",
    "*Major tracks in prediction:  4203 -> Part of dataset:  81.8 %*\n",
    "\n",
    "*Minor tracks in prediction:  935 -> Part of dataset:  18.2 %*\n",
    "\n",
    "We can use this knowledge and try to improve the score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  69.385 %\n",
      "Recall:  88.0829 %\n"
     ]
    }
   ],
   "source": [
    "xgb_tree = xgb.XGBClassifier(scale_pos_weight = 1.01)\n",
    "get_scores(xgb_tree, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding *scale_pos_weight* parameter didn't improve the score much (0.05%). Let us try decreasing number of records of more classes in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 17210 entries, 11827 to 21775\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   danceability      17210 non-null  float64\n",
      " 1   energy            17210 non-null  float64\n",
      " 2   key               17210 non-null  int64  \n",
      " 3   mode              17210 non-null  float64\n",
      " 4   loudness          17210 non-null  float64\n",
      " 5   speechiness       17210 non-null  float64\n",
      " 6   acousticness      17210 non-null  float64\n",
      " 7   instrumentalness  17210 non-null  float64\n",
      " 8   liveness          17210 non-null  float64\n",
      " 9   valence           17210 non-null  float64\n",
      " 10  tempo             17210 non-null  float64\n",
      " 11  time_signature    17210 non-null  int64  \n",
      "dtypes: float64(10), int64(2)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "df_0 = df[df['mode'] == 0]\n",
    "df_1 = df[df['mode'] == 1]\n",
    "diff = len(df_1.index) - len(df_0.index)\n",
    "df_1 = df_1.sample(frac=1).iloc[diff:]\n",
    "df_conc = pd.concat([df_1, df_0])\n",
    "df_conc = df_conc.sample(frac=1)\n",
    "df_conc.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  60.4881 %\n",
      "Recall:  61.9792 %\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, X_final, y_final = split(df_conc)\n",
    "\n",
    "xgb_tree = xgb.XGBClassifier()\n",
    "get_scores(xgb_tree, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you can see here is balancing the classes manually. In our case this move drastically decreased number of instances in the dataset. Client provided this dataset and no more rows are available, so we can't increase its size by loading more data. Cutting random rows to make number of instances of each class equal resulted with much smaller dataset, which was not enough for the model to learn."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing 'key' column structure\n",
    "\n",
    "Key column is now a numeric column. It shouldn't - key values are not linear and we can't sort them. First approach would be to create dummies out of this column.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  69.0152 %\n",
      "Recall:  87.3921 %\n"
     ]
    }
   ],
   "source": [
    "df_dummies = pd.get_dummies(df, columns=['key'])\n",
    "X_train, X_test, y_train, y_test, X_final, y_final = split(df_dummies)\n",
    "xgb_tree = xgb.XGBClassifier()\n",
    "get_scores(xgb_tree, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 12 different key values. It is not a small number, so our model doesn't cope with this type of data better than the standard one. We will try a different idea. We will change 'key' value to truly linear form. We will sort the keys by balance between minor and major tracks appearing in exact key. We hope that will affect our model to differentiate between keys containing more minor than major instances from those which contain a reversed balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F#</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>C#</th>\n",
       "      <th>A#</th>\n",
       "      <th>G#</th>\n",
       "      <th>E</th>\n",
       "      <th>G</th>\n",
       "      <th>B</th>\n",
       "      <th>A</th>\n",
       "      <th>F</th>\n",
       "      <th>D#</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>major</th>\n",
       "      <td>616.000000</td>\n",
       "      <td>2630.000000</td>\n",
       "      <td>2252.000000</td>\n",
       "      <td>1182.00000</td>\n",
       "      <td>938.000000</td>\n",
       "      <td>1162.00000</td>\n",
       "      <td>1081.000000</td>\n",
       "      <td>2552.000000</td>\n",
       "      <td>681.000000</td>\n",
       "      <td>1918.000000</td>\n",
       "      <td>1418.000000</td>\n",
       "      <td>653.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minor</th>\n",
       "      <td>639.000000</td>\n",
       "      <td>691.000000</td>\n",
       "      <td>722.000000</td>\n",
       "      <td>539.00000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>287.00000</td>\n",
       "      <td>1135.000000</td>\n",
       "      <td>724.000000</td>\n",
       "      <td>1021.000000</td>\n",
       "      <td>1169.000000</td>\n",
       "      <td>852.000000</td>\n",
       "      <td>234.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>balance</th>\n",
       "      <td>0.964006</td>\n",
       "      <td>3.806078</td>\n",
       "      <td>3.119114</td>\n",
       "      <td>2.19295</td>\n",
       "      <td>1.584459</td>\n",
       "      <td>4.04878</td>\n",
       "      <td>0.952423</td>\n",
       "      <td>3.524862</td>\n",
       "      <td>0.666993</td>\n",
       "      <td>1.640719</td>\n",
       "      <td>1.664319</td>\n",
       "      <td>2.790598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 F#            C            D          C#          A#   \n",
       "major    616.000000  2630.000000  2252.000000  1182.00000  938.000000  \\\n",
       "minor    639.000000   691.000000   722.000000   539.00000  592.000000   \n",
       "balance    0.964006     3.806078     3.119114     2.19295    1.584459   \n",
       "\n",
       "                 G#            E            G            B            A   \n",
       "major    1162.00000  1081.000000  2552.000000   681.000000  1918.000000  \\\n",
       "minor     287.00000  1135.000000   724.000000  1021.000000  1169.000000   \n",
       "balance     4.04878     0.952423     3.524862     0.666993     1.640719   \n",
       "\n",
       "                   F          D#  \n",
       "major    1418.000000  653.000000  \n",
       "minor     852.000000  234.000000  \n",
       "balance     1.664319    2.790598  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_dict = {0: \"C\", 1: \"C#\", 2: \"D\", 3: \"D#\", 4: \"E\", 5: \"F\", 6: \"F#\", 7: \"G\", 8: \"G#\", 9: \"A\", 10: \"A#\", 11: \"B\"}\n",
    "df[\"key\"].replace(letter_dict, inplace=True)\n",
    "\n",
    "def get_keys_mode_df():\n",
    "    keys_dict = {}\n",
    "    for index, row in df[[\"key\", \"mode\"]].iterrows():\n",
    "        if row[\"key\"] not in keys_dict.keys():\n",
    "            keys_dict[row[\"key\"]] = {\"major\": 0, \"minor\": 0, \"balance\": 0}\n",
    "        if row[\"mode\"] == 0:\n",
    "            keys_dict[row[\"key\"]][\"minor\"] += 1\n",
    "        if row[\"mode\"] == 1:\n",
    "            keys_dict[row[\"key\"]][\"major\"] += 1\n",
    "    for key in keys_dict:\n",
    "        keys_dict[key]['balance'] = keys_dict[key]['major'] / keys_dict[key]['minor'] \n",
    "    return pd.DataFrame.from_dict(keys_dict), keys_dict\n",
    "\n",
    "df_key, keys_dict = get_keys_mode_df()\n",
    "df_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', 0.6669931439764937),\n",
       " ('E', 0.9524229074889868),\n",
       " ('F#', 0.9640062597809077),\n",
       " ('A#', 1.5844594594594594),\n",
       " ('A', 1.6407185628742516),\n",
       " ('F', 1.664319248826291),\n",
       " ('C#', 2.1929499072356213),\n",
       " ('D#', 2.7905982905982905),\n",
       " ('D', 3.119113573407202),\n",
       " ('G', 3.5248618784530388),\n",
       " ('C', 3.806078147612156),\n",
       " ('G#', 4.048780487804878)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "keys = []\n",
    "balances = []\n",
    "for key in keys_dict:\n",
    "    keys.append(key)\n",
    "    balances.append(keys_dict[key]['balance'])\n",
    "\n",
    "sorted_keys = sorted(\n",
    "    list(zip(keys, balances)),\n",
    "    key=lambda t: t[1]\n",
    ")\n",
    "\n",
    "sorted_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B': 0, 'E': 1, 'F#': 2, 'A#': 3, 'A': 4, 'F': 5, 'C#': 6, 'D#': 7, 'D': 8, 'G': 9, 'C': 10, 'G#': 11}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "changing_dict = {}\n",
    "for index, pair in enumerate(sorted_keys):\n",
    "    changing_dict[pair[0]] = index\n",
    "print(changing_dict)\n",
    "\n",
    "df[\"key\"].replace(changing_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  68.4897 %\n",
      "Recall:  86.3846 %\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, X_final, y_final = split(df, 'advanced.bin')\n",
    "xgb_tree = xgb.XGBClassifier()\n",
    "get_scores(xgb_tree, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hoped that this operation will bring improvement. As we can observe, score is quite similar to previous ones. We decided keep it for further development anyway. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning\n",
    "\n",
    "This part is our main field for improvemt. We will try to get most of our data by changing values of the parameters. Considering tiny amount of time that xgboost needed for training, we conducted a lot of experiments changing parameters manually, seeking for best ones. Example below shows ranges between which we searched for optimal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators_n = [1, 20, 50, 90, 120]\n",
    "# learning_rates = [0.01, 0.03, 0.05, 0.1, 0.3, 0.7]\n",
    "\n",
    "# for n_estimator, learning_rate in zip(n_estimators_n, learning_rates):\n",
    "#     xgb_tree = xgb.XGBClassifier(\n",
    "#         n_estimators = n_estimator,\n",
    "#         learning_rate = learning_rate\n",
    "#     )\n",
    "#     print(\"N_estimators: \", n_estimator)\n",
    "#     print(\"Learning rate: \", learning_rate)\n",
    "#     get_scores(xgb_tree, X_train, y_train, X_test, y_test)\n",
    "#     print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best params chosen:\n",
    "- n_estimators: 90\n",
    "- learning rate: 0.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the model\n",
    "\n",
    "No matter what we did, our score didn't improve too much, that's why we decided to try a different model. Decision was made to choose SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# svc = SVC()\n",
    "# get_scores(svc, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appeared, that using SVC result with better basic accuracy and overall it didn't differ much from XGBoost. Disadvantage of this model is time needed to train it - while XGBoost Classifier needs maximum of 1.5 seconds to learn, SVC needs around 30. That's why our final model will be XGBoost with parameters chosen earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cs = [0.01, 1, 10]\n",
    "# degrees = [2, 3]\n",
    "# for c in cs:\n",
    "#     for d in degrees:\n",
    "#         svc = SVC(C=c, degree=d)\n",
    "#         print(\"C: \", c, \" Degree: \", d)\n",
    "#         get_scores(svc, X_train, y_train, X_test, y_test)\n",
    "#         print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  70.7474 %\n",
      "Recall:  91.019 %\n",
      "Cross validation scores:  [0.68956793 0.69540677 0.6969638  0.69398482 0.69807281]\n"
     ]
    }
   ],
   "source": [
    "final_model = xgb.XGBClassifier(\n",
    "        n_estimators = 90,\n",
    "        learning_rate = 0.1\n",
    "    )\n",
    "\n",
    "get_scores(xgb.XGBClassifier(n_estimators = 90, learning_rate = 0.1), X_train, y_train, X_test, y_test)\n",
    "get_cross_val_scores(xgb.XGBClassifier(n_estimators = 90, learning_rate = 0.1), X_final, y_final, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.fit(X_train, y_train)\n",
    "final_model.save_model(\"advanced_model.json\")\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[13:06:34] ../src/common/io.cc:102: Opening model.json failed: No such file or directory\nStack trace:\n  [bt] (0) /home/yamroza/anaconda3/envs/ml/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x194383) [0x7f4235b94383]\n  [bt] (1) /home/yamroza/anaconda3/envs/ml/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x194ae3) [0x7f4235b94ae3]\n  [bt] (2) /home/yamroza/anaconda3/envs/ml/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x144770) [0x7f4235b44770]\n  [bt] (3) /home/yamroza/anaconda3/envs/ml/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(XGBoosterLoadModel+0x185) [0x7f4235b44b55]\n  [bt] (4) /home/yamroza/anaconda3/envs/ml/lib/python3.11/lib-dynload/../../libffi.so.8(+0x6a4a) [0x7f429bdcaa4a]\n  [bt] (5) /home/yamroza/anaconda3/envs/ml/lib/python3.11/lib-dynload/../../libffi.so.8(+0x5fea) [0x7f429bdc9fea]\n  [bt] (6) /home/yamroza/anaconda3/envs/ml/lib/python3.11/lib-dynload/_ctypes.cpython-311-x86_64-linux-gnu.so(+0x92c4) [0x7f429bdd92c4]\n  [bt] (7) /home/yamroza/anaconda3/envs/ml/lib/python3.11/lib-dynload/_ctypes.cpython-311-x86_64-linux-gnu.so(+0x8816) [0x7f429bdd8816]\n  [bt] (8) /home/yamroza/anaconda3/envs/ml/bin/python(_PyObject_MakeTpCall+0x264) [0x560e82db3d34]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_model \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mXGBClassifier()\n\u001b[0;32m----> 2\u001b[0m test_model\u001b[39m.\u001b[39;49mload_model(\u001b[39m\"\u001b[39;49m\u001b[39mmodel.json\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m pred \u001b[39m=\u001b[39m test_model\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m      4\u001b[0m acc \u001b[39m=\u001b[39m accuracy_score(y_test, pred)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.11/site-packages/xgboost/sklearn.py:777\u001b[0m, in \u001b[0;36mXGBModel.load_model\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_Booster\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    776\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m Booster({\u001b[39m\"\u001b[39m\u001b[39mn_jobs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs})\n\u001b[0;32m--> 777\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_booster()\u001b[39m.\u001b[39;49mload_model(fname)\n\u001b[1;32m    778\u001b[0m meta_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_booster()\u001b[39m.\u001b[39mattr(\u001b[39m\"\u001b[39m\u001b[39mscikit_learn\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    779\u001b[0m \u001b[39mif\u001b[39;00m meta_str \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    780\u001b[0m     \u001b[39m# FIXME(jiaming): This doesn't have to be a problem as most of the needed\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[39m# information like num_class and objective is in Learner class.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.11/site-packages/xgboost/core.py:2444\u001b[0m, in \u001b[0;36mBooster.load_model\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m   2440\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fname, (\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike)):\n\u001b[1;32m   2441\u001b[0m     \u001b[39m# assume file name, cannot use os.path.exist to check, file can be\u001b[39;00m\n\u001b[1;32m   2442\u001b[0m     \u001b[39m# from URL.\u001b[39;00m\n\u001b[1;32m   2443\u001b[0m     fname \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexpanduser(fname))\n\u001b[0;32m-> 2444\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterLoadModel(\n\u001b[1;32m   2445\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, c_str(fname)))\n\u001b[1;32m   2446\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(fname, \u001b[39mbytearray\u001b[39m):\n\u001b[1;32m   2447\u001b[0m     buf \u001b[39m=\u001b[39m fname\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.11/site-packages/xgboost/core.py:279\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \n\u001b[1;32m    270\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[39m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [13:06:34] ../src/common/io.cc:102: Opening model.json failed: No such file or directory\nStack trace:\n  [bt] (0) /home/yamroza/anaconda3/envs/ml/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x194383) [0x7f4235b94383]\n  [bt] (1) /home/yamroza/anaconda3/envs/ml/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x194ae3) [0x7f4235b94ae3]\n  [bt] (2) /home/yamroza/anaconda3/envs/ml/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x144770) [0x7f4235b44770]\n  [bt] (3) /home/yamroza/anaconda3/envs/ml/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(XGBoosterLoadModel+0x185) [0x7f4235b44b55]\n  [bt] (4) /home/yamroza/anaconda3/envs/ml/lib/python3.11/lib-dynload/../../libffi.so.8(+0x6a4a) [0x7f429bdcaa4a]\n  [bt] (5) /home/yamroza/anaconda3/envs/ml/lib/python3.11/lib-dynload/../../libffi.so.8(+0x5fea) [0x7f429bdc9fea]\n  [bt] (6) /home/yamroza/anaconda3/envs/ml/lib/python3.11/lib-dynload/_ctypes.cpython-311-x86_64-linux-gnu.so(+0x92c4) [0x7f429bdd92c4]\n  [bt] (7) /home/yamroza/anaconda3/envs/ml/lib/python3.11/lib-dynload/_ctypes.cpython-311-x86_64-linux-gnu.so(+0x8816) [0x7f429bdd8816]\n  [bt] (8) /home/yamroza/anaconda3/envs/ml/bin/python(_PyObject_MakeTpCall+0x264) [0x560e82db3d34]\n\n"
     ]
    }
   ],
   "source": [
    "test_model = xgb.XGBClassifier()\n",
    "test_model.load_model(\"model.json\")\n",
    "pred = test_model.predict(X_test)\n",
    "acc = accuracy_score(y_test, pred)\n",
    "recall = recall_score(y_test, pred)\n",
    "print(\"Accuracy: \", round(acc*100, 4), \"%\")\n",
    "print(\"Recall: \", round(recall*100, 4), \"%\")\n",
    "# get_scores(test_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal of obtaining 70% accuracy is accomplished by using this hyperparameter set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monopoly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
